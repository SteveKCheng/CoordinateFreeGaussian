#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\bottommargin 1.25in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Coordinate-free characterization of the multi-variate Gaussian distribution
\end_layout

\begin_layout Author
Steve Cheng
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
This article introduces the theory of the multi-variate Gaussian (normal)
 probability distribution.
 In contrast to most textbooks
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "Wichura"
literal "false"

\end_inset

 is a wonderful exception that has inspired this work.
\end_layout

\end_inset

 on probability and statistics (elementary or not), we strive to work in
 a coordinate-free manner, using the full power of modern linear algebra.
 
\end_layout

\begin_layout Standard
In my own past experience as a university student, linear algebra presented
 as a theory of vector spaces gets introduced early in the curriculum for
 a mathematics major — starting in the first year, finishing in the second
 year of undergraduate study at the latest — yet the teaching of the application
s (e.g.
 physics, statistics) is usually geared to other students not majoring in
 mathematics, and so do not use the more abstract theory of vector spaces,
 preferring to work with (list) vectors and matrices.
 
\end_layout

\begin_layout Standard
I have always found that dissatisfying — and partly for that reason had
 learned a lot of statistics for a long time by re-deriving key results
 myself.
 Nowadays, I would argue that even in applications, the geometric perspective
 underlying vector-space theory is enormously helpful in reasoning through
 situations more complicated than in the toy problems of textbooks.
 Without a rigorous theory, geometric intuition might just be figments of
 the mind that occasionally lead us to mistakes.
 And, when we want to compute results, invariably programming a computer
 to do so, I think it is often helpful to thoroughly understand the meanings
 of quantities we want to compute, derive any needed analytics in the most
 generic form as is possible and practical — thus helping with code re-use
 — 
\emph on
before
\emph default
 assigning coordinates and indices to work with the vector and matrix representa
tions of those quantities.
\end_layout

\begin_layout Standard
This is an article I wish I could have read when I was an undergraduate.
 Not only because the applications are explained with more elegant techniques
 (in my not-so-humble opinion), but also it might help to impart intuition
 behind concepts like adjoints (from linear algebra), or the connection
 between orthogonality (in certain inner product spaces) and stochastic
 independence — which often go under-explained in pure-mathematics textbooks
 too.
\end_layout

\begin_layout Standard
Besides linear algebra, the background needed to read this article include
 some basic single-variable calculus, and elementary knowledge of probability
 theory.
 Unfortunately, strictly speaking, 
\begin_inset Quotes eld
\end_inset

elementary
\begin_inset Quotes erd
\end_inset

 probability theory is not enough, as we depend on certain well-known fundamenta
l facts, like the unique association of probability measures with their
 characteristic functions, that require serious learning of real analysis
 and the Kolmogorov axiomatization of probability theory
\begin_inset Foot
status open

\begin_layout Plain Layout
A rapid yet rigorous development can be found in 
\begin_inset CommandInset citation
LatexCommand cite
key "Rosenthal"
literal "false"

\end_inset

, which should be approachable by an undergraduate who has studied rigorous
 calculus.
\end_layout

\end_inset

 before they can really be established and understood.
 The reader without this knowledge will just have to treat such facts as
 given — but should still be able to follow the prose and algebraic manipulation
s.
 
\end_layout

\begin_layout Section
Basic definition
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $V$
\end_inset

 be a real vector space
\begin_inset Foot
status open

\begin_layout Plain Layout
The definition in this section need not be restricted to finite-dimensional
 vector spaces, but all the following sections require finite-dimensionality
 to work.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
A random variable 
\begin_inset Formula $Z$
\end_inset

 taking values in 
\begin_inset Formula $V$
\end_inset

 is said to be 
\emph on
normal
\emph default
, or 
\emph on
Gaussian
\emph default
, if for every linear functional 
\begin_inset Formula $\varphi$
\end_inset

, the real-valued random variable 
\begin_inset Formula $\varphi Z$
\end_inset

 has the one-dimensional Gaussian distribution, including the de-generate
 case that it is a constant.
\end_layout

\begin_layout Standard
Thus 
\begin_inset Formula $Z$
\end_inset

 is Gaussian if and only if, for all linear functionals 
\begin_inset Formula $\varphi$
\end_inset

, the characteristic function of 
\begin_inset Formula $\varphi Z$
\end_inset

 has the form:
\begin_inset Formula 
\[
f_{\varphi Z}(t)=\mathbb{E}\bigl[\exp(it\varphi Z)\bigr]=\exp(i\mu t-\tfrac{1}{2}\sigma^{2}t^{2})\,.
\]

\end_inset

Since any characteristic function 
\begin_inset Formula $f$
\end_inset

 of a real-valued, square-integrable random variable 
\begin_inset Formula $X$
\end_inset

 satisfies:
\begin_inset Formula 
\begin{align*}
f(0) & =1\,,\\
f'(0) & =\left.\frac{d}{dt}\right|_{t=0}\mathbb{E}\bigl[\exp(itX)\bigr]=\mathbb{E}\bigl[iX\exp(itX)\bigr]\mid_{t=0}=i\mathbb{E}X\,,\\
f''(0) & =\left.\frac{d^{2}}{dt^{2}}\right|_{t=0}\mathbb{E}\bigl[\exp(itX)\bigr]=\mathbb{E}\bigl[(iX)^{2}\exp(itX)\bigr]\mid_{t=0}=-\mathbb{E}X^{2}\,,
\end{align*}

\end_inset

by differentiating 
\begin_inset Formula $f_{\varphi Z}$
\end_inset

 we see that its parameters must necessarily satisfy: 
\begin_inset FormulaMacro
\newcommand{\Cov}{\operatorname{Cov}}
{\mathrm{Cov}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\Var}{\operatorname{Var}}
{\mathbb{\mathrm{Var}}}
\end_inset


\begin_inset Formula 
\begin{align*}
\mu & =\mathbb{E}\bigl[\varphi Z\bigr]\,,\\
\mu^{2}+\sigma^{2} & =\mathbb{E}\bigl[(\varphi Z)^{2}\bigr]\,,\quad\text{i.e. }\sigma^{2}=\Var(\varphi Z)\,.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Variance and covariance
\end_layout

\begin_layout Standard
In the following, we further assume that 
\begin_inset Formula $V=\mathbb{R}^{n}$
\end_inset

, equipped with the standard inner product of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
 Recall that the inner product induces a natural isomorphism between 
\begin_inset Formula $V$
\end_inset

 and its dual space 
\begin_inset Formula $V^{*}$
\end_inset

.
 Every 
\begin_inset Formula $\varphi\in V^{*}$
\end_inset

 is of the form 
\begin_inset Formula $\varphi(u)=\langle u,v\rangle$
\end_inset

 for some 
\begin_inset Formula $v\in V$
\end_inset

.
\end_layout

\begin_layout Standard
The 
\emph on
covariance
\emph default
 of a random variable 
\begin_inset Formula $Z$
\end_inset

 taking values in 
\begin_inset Formula $V$
\end_inset

 is defined to be the symmetric bilinear form:
\begin_inset Formula 
\[
\mathcal{C}(u,v)=\Cov\bigl[\langle u,Z\rangle,\langle v,Z\rangle\bigr]\,,\quad u,v\in V\,.
\]

\end_inset

The 
\emph on
variance
\emph default
 is the corresponding quadratic form:
\begin_inset Formula 
\[
\mathcal{C}(u)=\mathcal{C}(u,u)=\Var\bigl[\langle u,Z\rangle\bigr]\,,\quad u\in V\,.
\]

\end_inset


\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $\mathcal{C}$
\end_inset

, we may derive the exact form of the characteristic function of 
\begin_inset Formula $Z$
\end_inset

 as follows.
 Recall that such a characteristic function (over 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

) is defined as:
\begin_inset Formula 
\[
f_{Z}(\xi)=\mathbb{E}\bigl[\exp(i\langle\xi,Z\rangle)\bigr]\,,\quad\xi\in\mathbb{R}^{n}.
\]

\end_inset

First assume 
\begin_inset Formula $\mathbb{E}[Z]=0$
\end_inset

, so that 
\begin_inset Formula $\mathbb{E}[\varphi Z]=0$
\end_inset

 for all 
\begin_inset Formula $\varphi\in V^{*}$
\end_inset

.
 Now consider the characteristic function of 
\begin_inset Formula $\varphi Z$
\end_inset

 where 
\begin_inset Formula $u\in V$
\end_inset

 is the corresponding element to 
\begin_inset Formula $\varphi\in V^{*}$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is Gaussian, by definition 
\begin_inset Formula $\varphi Z=\langle u,Z\rangle$
\end_inset

 is Gaussian, and 
\begin_inset Formula 
\[
f_{\varphi Z}(t)=\mathbb{E}\bigl[\exp(it\langle u,Z\rangle)\bigr]=\exp(-\tfrac{1}{2}\sigma_{u}^{2}t^{2})\,,\quad\sigma_{u}^{2}=\mathbb{E}\bigl[\langle u,Z\rangle^{2}\bigr]=\mathcal{C}(u)\,.
\]

\end_inset

In particular set 
\begin_inset Formula $t=1$
\end_inset

 and 
\begin_inset Formula $u=\xi$
\end_inset

, and we obtain:
\begin_inset Formula 
\[
f_{Z}(\xi)=\exp\bigl(-\tfrac{1}{2}\mathcal{C}(\xi)\bigr)\,.
\]

\end_inset

If 
\begin_inset Formula $\mathbb{E}[Z]=z_{0}\neq0$
\end_inset

, applying the preceding special case to the zero-mean random variable 
\begin_inset Formula $Z-z_{0}$
\end_inset

 shows:
\begin_inset Formula 
\begin{align*}
f_{Z}(\xi) & =\mathbb{E}\bigl[\exp(i\langle\xi,Z-z_{0}+z_{0}\rangle)\bigr]=\exp\bigl(i\langle\xi,z_{0}\rangle\bigr)\mathbb{E}[\exp(i\langle\xi,Z-z_{0}\rangle)\bigr]\\
 & =\exp\bigl(i\langle\xi,z_{0}\rangle-\tfrac{1}{2}\mathcal{C}(\xi)\bigr)\,.
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Covariance operator and its induced inner product
\end_layout

\begin_layout Standard
By the natural isomorphism between 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $V^{*}$
\end_inset

, there exists a vector 
\begin_inset Formula $Cv$
\end_inset

 such that
\begin_inset Formula 
\[
\mathcal{C}(u,v)=\langle u,Cv\rangle\,.
\]

\end_inset

Since 
\begin_inset Formula $\mathcal{C}(u,v)$
\end_inset

 is linear in 
\begin_inset Formula $v$
\end_inset

, 
\begin_inset Formula $Cv$
\end_inset

 is also linear in 
\begin_inset Formula $v$
\end_inset

.
 Thus there exists a linear operator 
\begin_inset Formula $C\colon V\to V$
\end_inset

 such that
\begin_inset Formula 
\[
\Cov\bigl[\langle u,Z\rangle,\langle v,Z\rangle\bigr]=\langle u,Cv\rangle\,.
\]

\end_inset

Since the left quantity is symmetric in 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

, the linear operator 
\begin_inset Formula $C$
\end_inset

 must be self-adjoint.
 It is also positive semi-definite, because 
\begin_inset Formula $\mathcal{C}(u,u)\geq0$
\end_inset

 for all 
\begin_inset Formula $u\in V$
\end_inset

.
 
\end_layout

\begin_layout Standard
We call 
\begin_inset Formula $C$
\end_inset

 the 
\emph on
covariance operator
\emph default
; its matrix representation under the standard basis of 
\begin_inset Formula $V$
\end_inset

 is the usual 
\begin_inset Quotes eld
\end_inset

covariance matrix
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Consequently, the characteristic function of 
\begin_inset Formula $Z$
\end_inset

 may be re-written as:
\begin_inset Formula 
\[
f_{Z}(\xi)=\exp\bigl(i\langle\xi,z_{0}\rangle-\tfrac{1}{2}\langle\xi,C\xi\rangle\bigr)=\exp\bigl(i\langle\xi,z_{0}\rangle-\tfrac{1}{2}\left\Vert \xi\right\Vert _{C}^{2}\bigr)\,.
\]

\end_inset

The function 
\begin_inset Formula $\xi\mapsto\sqrt{\langle\xi,C\xi\rangle}$
\end_inset

 is a semi-norm which we may denote by 
\begin_inset Formula $\lVert\cdot\rVert_{C}$
\end_inset

, determined by the 
\begin_inset Quotes eld
\end_inset

covariance structure
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathcal{C}$
\end_inset

 of 
\begin_inset Formula $Z$
\end_inset

.
 
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $C$
\end_inset

 is non-singular, 
\begin_inset Formula $\mathcal{C}(u,v)=\langle u,Cv\rangle$
\end_inset

 defines another (positive-definite) inner product on 
\begin_inset Formula $V$
\end_inset

, and 
\begin_inset Formula $\lVert\cdot\rVert_{C}$
\end_inset

 becomes a norm.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:linear-transformations"

\end_inset

Linear transformations
\end_layout

\begin_layout Standard
We may also consider the random variable 
\begin_inset Formula $LZ$
\end_inset

 defined by an arbitrary linear transformation 
\begin_inset Formula $L\colon V\to W$
\end_inset

, where 
\begin_inset Formula $W=\mathbb{R}^{m}$
\end_inset

 with its standard inner product.
 The characteristic function of 
\begin_inset Formula $LZ$
\end_inset

 would be:
\begin_inset Formula 
\begin{align*}
f_{LZ}(\xi)=\mathbb{E}\bigl[\exp(-i\langle\xi,LZ\rangle)\bigr]=\mathbb{E}\bigl[\exp(-i\langle L^{*}\xi,Z\rangle)\bigr] & =f_{Z}(L^{*}\xi)\\
 & =\exp\bigl(i\langle L^{*}\xi,z_{0}\rangle-\tfrac{1}{2}\left\Vert L^{*}\xi\right\Vert _{C}^{2}\bigr)\\
 & =\exp\bigl(i\langle\xi,Lz_{0}\rangle-\tfrac{1}{2}\langle\xi,(L\mathrm{C}L^{*})\xi\rangle\bigr)\,,
\end{align*}

\end_inset

where 
\begin_inset Formula $L^{*}\colon W\to V$
\end_inset

 is the adjoint of 
\begin_inset Formula $L$
\end_inset

.
 
\end_layout

\begin_layout Standard
Evidently, the covariance operator of 
\begin_inset Formula $LZ$
\end_inset

 is 
\begin_inset Formula $LCL^{*}$
\end_inset

, which, of course, could have been derived directly from the definition
 of covariance.
 The transformation of the mean from 
\begin_inset Formula $\mathbb{E}Z=z_{0}$
\end_inset

 to 
\begin_inset Formula $\mathbb{E}[LZ]=Lz_{0}$
\end_inset

 is obvious.
\end_layout

\begin_layout Standard
Having an explicit formula for 
\begin_inset Formula $L^{*}\colon W\to V$
\end_inset

, in coordinate-free form, will be helpful in the following sections.
 To derive one, let 
\begin_inset Formula $L\colon V\to W$
\end_inset

 be expressed in the form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Lz=\sum_{k=1}^{m}\langle z,v_{k}\rangle_{V}\,e_{k}\,,\quad z\in V\,,
\]

\end_inset

where 
\begin_inset Formula $e_{1},\dotsc,e_{m}$
\end_inset

 form any orthonormal
\begin_inset Foot
status open

\begin_layout Plain Layout
We note this derivation works with any inner products assigned to 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

, although we are only concerned with the standard inner products here.
\end_layout

\end_inset

 basis of 
\begin_inset Formula $W$
\end_inset

, and 
\begin_inset Formula $v_{1},\dotsc,v_{m}\in V$
\end_inset

 are determined uniquely by: 
\begin_inset Formula 
\[
\langle z,v_{k}\rangle_{V}=\langle Lz,e_{k}\rangle_{W}\,,\quad\text{for all \ensuremath{z\in V}.}
\]

\end_inset

We distinguish the inner products on 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 by subscripts here, for clarity.
\end_layout

\begin_layout Standard
Applying the definition of the adjoint to the preceding equation:
\begin_inset Formula 
\[
\langle z,v_{k}\rangle_{V}=\langle z,L^{*}e_{k}\rangle_{W}\,,\quad\text{for all \ensuremath{z\in V}},
\]

\end_inset

we obtain immediately: 
\begin_inset Formula 
\[
L^{*}e_{k}=v_{k}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
And then, more generally, for any 
\begin_inset Formula $w\in W$
\end_inset

:
\begin_inset Formula 
\begin{align*}
L^{*}w=L^{*}\left(\sum_{k=1}^{n}\langle w,e_{k}\rangle_{W}\,e_{k}\right) & =\sum_{k=1}^{n}\langle w,e_{k}\rangle_{W}\,v_{k}\,.
\end{align*}

\end_inset

We may more easily remember this formula by seeing that the vectors 
\begin_inset Formula $v_{k}$
\end_inset

 and 
\begin_inset Formula $e_{j}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

switch places
\begin_inset Quotes erd
\end_inset

 in going from 
\begin_inset Formula $T$
\end_inset

 to 
\begin_inset Formula $T^{*}$
\end_inset

.
 
\end_layout

\begin_layout Section
Lack of correlation implies independence
\end_layout

\begin_layout Standard
The following sections describe two important transformations that generate
 such orthogonal, independent components.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:independence-orthogonal-1"

\end_inset

Orthogonal components from spectral decomposition
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $C$
\end_inset

 is self-adjoint, by the spectral theorem (in finite dimensions), it has
 a complete set of orthonormal eigenvectors 
\begin_inset Formula $v_{1},\dotsc,v_{n}\in V$
\end_inset

 (with respect to the standard inner product), with corresponding eigenvalues
 
\begin_inset Formula $\lambda_{k}\in\mathbb{R}$
\end_inset

: 
\begin_inset Formula 
\[
Cv_{k}=\lambda_{k}v_{k}\,.
\]

\end_inset

Also, the eigenvalues are non-negative since 
\begin_inset Formula $C$
\end_inset

 is positive semi-definite.
\end_layout

\begin_layout Standard
Assume again that 
\begin_inset Formula $\mathbb{E}Z=z_{0}=0$
\end_inset

.
 If we next consider the random variables 
\begin_inset Formula 
\[
X_{k}=\langle v_{k},Z\rangle\,,
\]

\end_inset

their 
\begin_inset Formula $n$
\end_inset

-dimensional joint distribution has the following characteristic function:
 
\begin_inset Formula 
\begin{align*}
f_{X}(\xi)=\mathbb{E}\left[\exp\left(i\sum_{k=1}^{n}\xi_{k}X_{k}\right)\right]=\mathbb{E}\left[\exp\left(i\sum_{k=1}^{n}\langle\xi_{k}v_{k},Z\rangle\right)\right] & =f_{Z}\left(\sum_{k=1}^{n}\xi_{k}v_{k}\right)\\
 & =\exp\left(-\frac{1}{2}\left\langle \sum_{j=1}^{n}\xi_{j}v_{j},C\sum_{k=1}^{n}\xi_{k}v_{k}\right\rangle \right)\\
 & =\text{\exp\left(-\frac{1}{2}\sum_{k=1}^{n}\lambda_{k}\xi_{k}^{2}\right)}\\
 & =\prod_{k=1}^{n}\exp\bigl(-\tfrac{1}{2}\lambda_{k}\xi_{k}^{2}\bigr)\,.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because 
\begin_inset Formula $f_{X}$
\end_inset

 factors into a product of characteristic functions for 
\begin_inset Formula $n$
\end_inset

 instances of the Gaussian distribution, 
\begin_inset Formula $X_{1},\dotsc,X_{n}$
\end_inset

 are independent random variables.
 Their respective variances are 
\begin_inset Formula 
\[
\lambda_{k}=\mathcal{C}(v_{k})=\langle v_{k},Cv_{k}\rangle\,.
\]

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:independence-orthogonal-2"

\end_inset

Orthogonal components from inverse mapping, scalar form
\end_layout

\begin_layout Standard
There exists another transformation of 
\begin_inset Formula $Z$
\end_inset

 whose components are independent.
 It will become useful in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:linear-regression"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
We start with a technical set up in case the covariance operator 
\begin_inset Formula $C$
\end_inset

 is singular.
 Intuitively, we just restrict it to a subspace where it is invertible.
 
\end_layout

\begin_layout Standard
To that end, let 
\begin_inset Formula $CV=W\subseteq V$
\end_inset

 be the range of 
\begin_inset Formula $C$
\end_inset

; if 
\begin_inset Formula $C$
\end_inset

 is singular then 
\begin_inset Formula $W\neq V$
\end_inset

.
 Restricting the domain of 
\begin_inset Formula $C$
\end_inset

 to 
\begin_inset Formula $W$
\end_inset

 makes it invertible, because 
\begin_inset Formula $W$
\end_inset

 equals the orthogonal complement (with respect to the standard inner product)
 of the null space of 
\begin_inset Formula $C$
\end_inset

.
 The last assertion comes from that fact, for 
\emph on
any
\emph default
 transformation 
\begin_inset Formula $T:V\to V'$
\end_inset

 between finite-dimensional inner product spaces:
\begin_inset Formula 
\begin{equation}
T^{*}V=\left(T^{-1}\{0\}\right)^{\perp}\,.\label{eq:adjoint-complement}
\end{equation}

\end_inset

And we substitute in 
\begin_inset Formula $T=C=T^{*}$
\end_inset

.
\end_layout

\begin_layout Standard
The subspace 
\begin_inset Formula $W$
\end_inset

 thus has an inner product induced by 
\begin_inset Formula $C^{-1}$
\end_inset

 restricted to 
\begin_inset Formula $W$
\end_inset

:
\begin_inset Formula 
\[
\langle v,w\rangle_{C^{-1}}=\langle v,C^{-1}w\rangle\,,\quad v,w\in W\,.
\]

\end_inset

Without change of notation, we extend the linear transformation 
\begin_inset Formula $C^{-1}\colon W\to V$
\end_inset

 to 
\begin_inset Formula $C^{-1}\colon V\to V$
\end_inset

 by setting 
\begin_inset Formula $C^{-1}$
\end_inset

 to be zero on 
\begin_inset Formula $W^{\perp}$
\end_inset

.
 Note that 
\begin_inset Formula $C^{-1}$
\end_inset

 extended
\begin_inset Foot
status open

\begin_layout Plain Layout
This extension is just the pseudo-inverse, but we hardly need to invoke
 the whole theory of pseudo-inverses here.
\end_layout

\end_inset

 this way remains self-adjoint.
 We continue to write 
\begin_inset Formula $\langle v,w\rangle_{C^{-1}}$
\end_inset

 for vectors 
\begin_inset Formula $v,w\in V\setminus W$
\end_inset

 even though 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

 may be only a positive semi-definite bilinear form over all of 
\begin_inset Formula $V$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $w_{1},\dotsc,w_{m}\in W$
\end_inset

 be an orthonormal basis, with respect to 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

.
 Consider the random variables 
\begin_inset Formula 
\[
Y_{k}=\langle w_{k},Z\rangle_{C^{-1}}=\langle C^{-1}w_{k},Z\rangle\,,
\]

\end_inset

Set 
\begin_inset Formula $L\colon V\to W$
\end_inset

 by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Lz=\sum_{k=1}^{m}\langle C^{-1}w_{k},z\rangle e_{k}\,,
\]

\end_inset

which implies
\begin_inset Formula 
\[
L^{*}e_{k}=C^{-1}w_{k}\,,
\]

\end_inset

from a key result of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:linear-transformations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Assuming 
\begin_inset Formula $\mathbb{E}Z=0$
\end_inset

, we may expand the characteristic function of the 
\begin_inset Formula $m$
\end_inset

-dimensional joint distribution of 
\begin_inset Formula $Y_{k}$
\end_inset

, in the same manner we did in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Formula 
\begin{align*}
f_{Y}(\xi)=\mathbb{E}\left[\exp\left(i\sum_{k=1}^{m}\xi_{k}Y_{k}\right)\right] & =\mathbb{E}\left[\exp\left(i\sum_{k=1}^{m}\langle\xi_{k}C^{-1}w_{k},Z\rangle\right)\right]\\
 & =f_{Z}\left(\sum_{k=1}^{m}\xi_{k}C^{-1}w_{k}\right)\\
 & =\exp\left(-\frac{1}{2}\left\langle \sum_{j=1}^{m}\xi_{j}C^{-1}w_{j},C\sum_{k=1}^{m}\xi_{k}C^{-1}w_{k}\right\rangle \right)\\
 & =\exp\left(-\frac{1}{2}\left\langle \sum_{j=1}^{m}\xi_{j}w_{j},\sum_{k=1}^{m}\xi_{k}w_{k}\right\rangle _{C^{-1}}\right)\\
 & =\prod_{k=1}^{m}\text{\exp\bigl(}-\tfrac{1}{2}\xi_{k}^{2}\bigr)\,.
\end{align*}

\end_inset

Again we have a factorization of the characteristic function, except this
 time the components have unit variance — there is no 
\begin_inset Formula $\lambda_{k}$
\end_inset

 in the above equation.
\end_layout

\begin_layout Standard
We have shown the random variables 
\begin_inset Formula $Y_{1},\dotsc,Y_{m}$
\end_inset

 to be independent.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:independence-orthogonal-3"

\end_inset

Orthogonal components from inverse mapping, vector form
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

inverse mapping
\begin_inset Quotes erd
\end_inset

 from the preceding section can be phrased in a more abstract way, more
 convenient for some analytical work.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $W$
\end_inset

 be orthogonally decomposed into subspaces 
\begin_inset Formula $M_{\ell}$
\end_inset

, for 
\begin_inset Formula $1\leq\ell\leq p$
\end_inset

, with the 
\begin_inset Formula $M_{k}$
\end_inset

 being chosen in any way as long as they are mutually orthogonal under 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

:
\begin_inset Formula 
\[
W=M_{1}\oplus M_{2}\oplus\dotsb\oplus M_{p}\,.
\]

\end_inset

If 
\begin_inset Formula $P_{M_{\ell}}$
\end_inset

 is the 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

-orthogonal projection to 
\begin_inset Formula $M_{\ell}$
\end_inset

, then the summands in:
\begin_inset Formula 
\[
Z=P_{M_{1}}(Z)+P_{M_{2}}(Z)+\dotsb+P_{M_{p}}(Z)
\]

\end_inset

ought to be independent, because they 
\begin_inset Quotes eld
\end_inset

live in
\begin_inset Quotes erd
\end_inset

 orthogonal linear subspaces, even if not aligned to the coordinate axes.
 
\end_layout

\begin_layout Standard
In the preceding proof of independence (from §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we had to work with scalar random variables, i.e.
 
\begin_inset Formula $Y_{k}$
\end_inset

, so that we can stack them up to form their joint probability law under
 
\begin_inset Formula $\mathbb{R}^{m}$
\end_inset

.
 And reducing everything to the standard inner product was essential because
 characteristic functions, i.e.
 the Fourier transform of probability laws, rely on the Euclidean structure
 of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
 Showing the vector-valued random variables 
\begin_inset Formula $P_{M_{\ell}}(Z)$
\end_inset

 are independent just requires making some transformations.
 There is nothing deep in the following demonstration, only mildly tedious
 bookkeeping.
\end_layout

\begin_layout Standard
Let the orthonormal basis 
\begin_inset Formula $w_{1},\dotsc,w_{m}\in W$
\end_inset

, from §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, be arranged so that the basis vectors for 
\begin_inset Formula $M_{1}$
\end_inset

 come first, followed by those for 
\begin_inset Formula $M_{2}$
\end_inset

, and so on up to 
\begin_inset Formula $M_{p}$
\end_inset

.
 Formally: 
\begin_inset FormulaMacro
\newcommand{\dim}{\operatorname{dim}}
{\mathbb{\mathrm{dim}}}
\end_inset


\begin_inset Formula 
\[
w_{k}\in M_{\ell}\quad\text{whenever }m_{\ell-1}<k\leq m_{\ell}\,,\quad m_{\ell}=\sum_{s=1}^{\ell}\dim M_{s}\,.
\]

\end_inset

Define the linear transformation 
\begin_inset Formula $T\colon\mathbb{R}^{m}\to W$
\end_inset

 by 
\begin_inset Formula $Te_{k}=w_{k}$
\end_inset

 for 
\begin_inset Formula $1\leq k\leq m$
\end_inset

, where 
\begin_inset Formula $e_{1},\dotsc,e_{m}\in\mathbb{R}^{m}$
\end_inset

 form the orthonormal basis under the standard inner product.
 Then 
\begin_inset Formula 
\[
T(\overline{Y}_{\ell})=\sum_{k=m_{\ell-1}}^{m_{\ell}}\langle w_{k},Z\rangle_{C^{-1}}\,w_{k}=P_{M_{\ell}}(Z)\in M_{\ell}\,,\quad\text{for}\quad\overline{Y}_{\ell}=\sum_{k=m_{\ell-1}}^{m_{\ell}}Y_{k}\,e_{k}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
With 
\begin_inset Formula $Y_{1},\dotsc,Y_{m}$
\end_inset

 being independent, it is clear (from the basic definition
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $\Pr(Y_{1}\in B_{1}\,,Y_{2}\in B_{2},\dotsb,Y_{m}\in B_{m})=\prod_{k=}^{m}\Pr(Y_{k}\in B_{k})$
\end_inset

 for all Borel sets 
\begin_inset Formula $B_{k}$
\end_inset

 on the corresponding codomain of 
\begin_inset Formula $Y_{k}$
\end_inset

.
\end_layout

\end_inset

 of independence of random variables), that 
\begin_inset Formula $Y_{1}e_{1},\dotsc,Y_{m}e_{m}$
\end_inset

 are independent also, and so are the vector sums 
\begin_inset Formula $\overline{Y}_{1},\dotsc,\overline{Y}_{p}$
\end_inset

 whose summands are disjoint.
 Then a transformation 
\begin_inset Formula $T$
\end_inset

 applied to each of 
\begin_inset Formula $\overline{Y}_{1},\dotsc,\overline{Y}_{p}$
\end_inset

 preserves independence, and, of course, that means 
\begin_inset Formula $P_{M_{1}}(Z),\dotsc,P_{M_{p}}(Z)$
\end_inset

 are independent.
\end_layout

\begin_layout Standard
The reader may be a little puzzled, as this author had been, that the inner
 product with respect to 
\begin_inset Formula $C^{-1}$
\end_inset

 rather than to 
\begin_inset Formula $C$
\end_inset

 is involved throughout here and in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 .
 It is essential; mapping through 
\begin_inset Formula $\langle,\rangle_{C}$
\end_inset

 does not work.
 To convince ourselves of that we shall demonstrate a certain relation with
 the 
\begin_inset Formula $C$
\end_inset

-orthogonal projections.
\end_layout

\begin_layout Standard
We first write down this obvious relation:
\begin_inset Formula 
\[
\langle w_{j},w_{k}\rangle_{C^{-1}}=\langle w_{j},C^{-1}w_{k}\rangle=\langle C^{-1}w_{j},CC^{-1}w_{k}\rangle=\langle v_{j},Cv_{k}\rangle=\langle v_{j},v_{k}\rangle_{C}\,,\quad w_{k}=Cv_{k}\,,
\]

\end_inset

so that the vectors 
\begin_inset Formula $w_{1},\dotsc,w_{m}$
\end_inset

 are 
\begin_inset Formula $C^{-1}$
\end_inset

-orthonormal if and only if 
\begin_inset Formula $v_{1},\dotsc,v_{m}$
\end_inset

 are 
\begin_inset Formula $C$
\end_inset

-orthonormal.
 
\begin_inset FormulaMacro
\newcommand{\linspan}{\operatorname{span}}
{\mathbb{\mathrm{span}}}
\end_inset

 We are led to define the map the subspaces by: 
\begin_inset Formula 
\[
M_{\ell}'=C^{-1}M_{\ell}=\linspan\big\{ v_{k}\colon m_{\ell-1}<k\leq m_{\ell}\bigr\}\,,
\]

\end_inset

and take the orthogonal projections 
\begin_inset Formula $Q_{M'_{\ell}}\colon W\to W$
\end_inset

 to 
\begin_inset Formula $M_{k}'$
\end_inset

 under inner product 
\begin_inset Formula $\langle,\rangle_{C}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Let us try to write 
\begin_inset Formula $P_{M_{\ell}}$
\end_inset

 in terms of 
\begin_inset Formula $Q_{M'_{\ell}}$
\end_inset

:
\begin_inset Formula 
\[
P_{M_{\ell}}(z)=\sum_{k=m_{\ell-1}}^{m_{\ell}}\langle z,w_{k}\rangle_{C^{-1}}\,w_{k}=\sum_{k=m_{\ell-1}}^{m_{\ell}}\bigl\langle z,C^{-1}Cv_{k}\bigr\rangle\,Cv_{k}=\sum_{k=m_{\ell-1}}^{m_{\ell}}\bigl\langle C^{-1}z,v_{k}\bigr\rangle_{C}\,Cv_{k}\,.
\]

\end_inset

More concisely: 
\begin_inset Formula 
\[
P_{M_{\ell}}=CQ_{M_{\ell}^{'}}C^{-1}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
This strange formula happens to have an interpretation, which requires a
 digression.
 We know that any orthogonal projection is self-adjoint — when the adjoint
 is based on the same inner product as on the projection, of course.
 What if we take an adjoint based on a different inner product? Then we
 need a 
\begin_inset Quotes eld
\end_inset

change of variables
\begin_inset Quotes erd
\end_inset

 formula for the adjoint.
 We derive it now for an arbitrary transformation 
\begin_inset Formula $T\colon V\to U$
\end_inset

 between inner product spaces, which will prove useful later too.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $T^{*}\colon U\to V$
\end_inset

 and 
\begin_inset Formula $T^{\logof}\colon U\to V$
\end_inset

 denote the adjoint of 
\begin_inset Formula $T$
\end_inset

 with respect to the inner products 
\begin_inset Formula $\langle,\rangle$
\end_inset

 and 
\begin_inset Formula $\langle,\rangle_{C}$
\end_inset

 respectively.
 Then starting from the definition of adjoints, for all 
\begin_inset Formula $u\in U$
\end_inset

 and 
\begin_inset Formula $v\in V$
\end_inset

 we have: 
\begin_inset Formula 
\[
\langle T^{\logof}u,v\rangle_{C}=\langle u,Tv\rangle_{C}=\langle u,CTv\rangle=\langle Cu,Tv\rangle=\langle T^{*}Cu,v\rangle=\langle C^{-1}T^{*}Cu,v\rangle_{C}\,.
\]

\end_inset

So we find that:
\begin_inset Formula 
\begin{equation}
T^{\logof}=C^{-1}T^{*}C\,,\quad\text{or}\quad T^{*}=CT^{\logof}C^{-1}\,.\label{eq:adjoint-change}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Setting 
\begin_inset Formula $T=Q_{M_{\ell}'}=T^{\logof}$
\end_inset

, we find that:
\begin_inset Formula 
\[
P_{M_{\ell}}=Q_{M_{\ell}'}^{*}\,.
\]

\end_inset

And the random variable 
\begin_inset Formula $Z$
\end_inset

 can be decomposed as:
\begin_inset Formula 
\[
Z=Q_{M_{1}'}^{*}(Z)+Q_{M_{2}'}^{*}(Z)+\dotsb+Q_{M_{p}'}^{*}(Z)\,,
\]

\end_inset

with the 
\begin_inset Formula $p$
\end_inset

 random variables on the right being stochastically independent.
\end_layout

\begin_layout Standard
There is no real advantage in decomposing with 
\begin_inset Formula $Q_{M_{\ell}^{'}}^{*}$
\end_inset

 versus 
\begin_inset Formula $P_{M_{\ell}}$
\end_inset

, since they are exactly the same transformation.
 We display 
\begin_inset Formula $Q_{M_{\ell}^{'}}^{*}$
\end_inset

 only as a theoretical curiosity.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:linear-regression"

\end_inset

Application: linear regression models
\end_layout

\begin_layout Standard
As an application, consider the linear regression model with data points
 in 
\begin_inset Formula $V$
\end_inset

:
\begin_inset Formula 
\[
Y=K\beta+\sigma\varepsilon\,.
\]

\end_inset

We take 
\begin_inset Formula $\varepsilon$
\end_inset

 to be a 
\begin_inset Formula $V$
\end_inset

-valued Gaussian random variable with zero mean, and with non-singular covarianc
e operator 
\begin_inset Formula $C\colon V\to V$
\end_inset

.
 The parameter vector 
\begin_inset Formula $\beta$
\end_inset

 lives in an 
\begin_inset Formula $m$
\end_inset

-dimensional real vector space 
\begin_inset Formula $W$
\end_inset

, while 
\begin_inset Formula $K\colon W\to V$
\end_inset

 is an injective linear transformation, and 
\begin_inset Formula $\sigma>0$
\end_inset

 is a real scalar.
 
\end_layout

\begin_layout Standard
Given the data points, i.e.
 realizations of the random variable 
\begin_inset Formula $Y$
\end_inset

, we would like to assume the above parametric model governing 
\begin_inset Formula $Y$
\end_inset

, though the parameters 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 are unknown and must be estimated from the data.
 
\end_layout

\begin_layout Standard
A usual choice is to estimate 
\begin_inset Formula $\beta$
\end_inset

 by minimizing the 
\begin_inset Quotes eld
\end_inset

sum of squares
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bigl\Vert Y-K\widetilde{\beta}\bigr\Vert_{C^{-1}}^{2}\,,\text{\ensuremath{\quad}over \widetilde{\beta} taking values in \ensuremath{W}. }
\]

\end_inset

Abstractly, the solution for 
\begin_inset Formula $K\widetilde{\beta}$
\end_inset

 is the orthogonal projection 
\begin_inset Formula $P_{M}$
\end_inset

 of 
\begin_inset Formula $Y$
\end_inset

 onto the image 
\begin_inset Formula $M=KW$
\end_inset

, with respect to the inner product induced by 
\begin_inset Formula $C^{-1}$
\end_inset

.
 In most applications, 
\begin_inset Formula $C$
\end_inset

 is the identity operator, so 
\begin_inset Formula $\lVert Y-K\widetilde{\beta}\rVert_{C^{-1}}^{2}$
\end_inset

 weights each squared residual equally.
 If 
\begin_inset Formula $C$
\end_inset

 is not the identity, then 
\begin_inset Formula $\lVert Y-K\widetilde{\beta}\rVert_{C^{-1}}^{2}$
\end_inset

 weights the principal components in inverse proportion to their intrinsic
 variances, so each component contributes 
\begin_inset Quotes eld
\end_inset

fairly
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Observe that 
\begin_inset Formula $I-P_{M}$
\end_inset

 is the orthogonal projection to the orthogonal complement 
\begin_inset Formula $M^{\perp}$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

, and 
\begin_inset Formula 
\[
Y=P_{M}Y+(I-P_{M})Y
\]

\end_inset

for all values of 
\begin_inset Formula $Y$
\end_inset

.
 Defining the random variable 
\begin_inset Formula $\widehat{\beta}=K^{-1}P_{M}Y$
\end_inset

, we find:
\begin_inset Formula 
\begin{align*}
\bigl\Vert Y-K\widetilde{\beta}\bigr\Vert_{C^{-1}}^{2} & =\bigl\Vert P_{M}Y-K\widetilde{\beta}+(I-P_{M})Y\bigr\Vert_{C^{-1}}^{2}\\
 & =\bigl\Vert P_{M}(Y-K\widetilde{\beta})+(I-P_{M})Y\bigr\Vert_{C^{-1}}^{2}\\
 & =\bigl\Vert P_{M}(Y-K\widetilde{\beta})\bigr\Vert_{C^{-1}}^{2}+\bigl\Vert(I-P_{M})Y\bigr\Vert_{C^{-1}}^{2}\\
 & =\bigl\Vert K(\widehat{\beta}-\widetilde{\beta})\bigr\Vert_{C^{-1}}^{2}+\bigl\Vert(I-P_{M})Y\bigr\Vert_{C^{-1}}^{2}\\
 & \geq\bigl\Vert(I-P_{M})Y\bigr\Vert_{C^{-1}}^{2}\,,
\end{align*}

\end_inset

with equality occurring if and only if 
\begin_inset Formula $\widetilde{\beta}=\widehat{\beta}$
\end_inset

.
 This justifies 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 being called the 
\begin_inset Quotes eld
\end_inset

least-squares estimator
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $\beta$
\end_inset

.
 It is an unbiased estimator, for:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}\bigl[K\widehat{\beta}\bigr]=\mathbb{E}\bigl[P_{M}Y\bigr]=P_{M}(\mathbb{E}Y)=P_{M}(K\beta)=K\beta\,.
\]

\end_inset


\end_layout

\begin_layout Standard
Also, 
\begin_inset Formula $\sigma^{-2}\bigl\Vert Y-K\widehat{\beta}\bigr\Vert_{C^{-1}}^{2}$
\end_inset

 is stochastically independent of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

, and has the chi-squared distribution with 
\begin_inset Formula $n-m$
\end_inset

 degrees of freedom.
 To see the first assertion, let 
\begin_inset Formula $v_{1},\dotsc,v_{m}$
\end_inset

 be an orthonormal basis (with respect to 
\begin_inset Formula $C^{-1}$
\end_inset

) for the subspace 
\begin_inset Formula $M\subseteq V$
\end_inset

, and 
\begin_inset Formula $v_{m+1},\dotsc,v_{n}$
\end_inset

 be an orthonormal basis for 
\begin_inset Formula $M^{\perp}\subseteq V$
\end_inset

.
 We write the orthogonal projections explicitly:
\begin_inset Formula 
\[
K\widehat{\beta}=P_{M}Y=\sum_{k=1}^{m}\langle v_{k},Y\rangle_{C^{-1}}v_{k}\,,\quad Y-K\widehat{\beta}=(I-P_{M})Y=\sum_{k=m+1}^{n}\langle v_{k},Y\rangle_{C^{-1}}v_{k}\,.
\]

\end_inset

The Gaussian random variables 
\begin_inset Formula $\langle\sigma^{-1}v_{k},Y\rangle_{C^{-1}}$
\end_inset

 are independent by the result of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, applied to the random variable 
\begin_inset Formula $Y$
\end_inset

 which has covariance 
\begin_inset Formula $\sigma^{2}C$
\end_inset

.
 So, 
\begin_inset Formula $K\widehat{\beta}$
\end_inset

 and 
\begin_inset Formula $Y-K\widehat{\beta},$
\end_inset

 being weighted sums on disjoint 
\begin_inset Formula $\langle v_{k},Y\rangle_{C^{-1}}$
\end_inset

, must be independent too.
 
\end_layout

\begin_layout Standard
Additionally, by the same result of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula 
\[
\frac{1}{\sigma^{2}}\bigl\Vert Y-K\widehat{\beta}\bigr\Vert_{C^{-1}}^{2}=\sum_{k=m+1}^{n}\bigl\langle\sigma^{-1}v_{k},Y\bigr\rangle{}_{C^{-1}}^{2}
\]

\end_inset

is a sum of 
\begin_inset Formula $n-m$
\end_inset

 independent Gaussian random variables of variance 
\begin_inset Formula $1$
\end_inset

.
 Provided that the means of those Gaussians are zero, the sum is distributed
 as 
\begin_inset Formula $\chi^{2}(n-m)$
\end_inset

.
 That 
\begin_inset Formula $\langle\sigma^{-1}v_{k},Y\rangle_{C^{-1}}$
\end_inset

 has zero mean for 
\begin_inset Formula $k>m$
\end_inset

 comes from:
\begin_inset Formula 
\begin{align*}
\mathbb{E}\bigl[\langle v_{k},Y\rangle_{C^{-1}}\bigr] & =\mathbb{E}\left[\bigl\langle v_{k},K\widehat{\beta}\bigr\rangle{}_{C^{-1}}+\bigl\langle v_{k},Y-K\widehat{\beta}\bigr\rangle{}_{C^{-1}}\right]\\
 & =\mathbb{E}\left[\bigl\langle v_{k},P_{M}Y\bigr\rangle_{C^{-1}}\right]+\bigl\langle v_{k},\mathbb{E}\bigl[Y-K\widehat{\beta}\bigr]\bigr\rangle_{C^{-1}}=0+0\,.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Finally, the estimate of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 from the data is, naturally:
\begin_inset Formula 
\[
\widehat{\sigma}^{2}=\frac{1}{n-m}\bigl\Vert Y-K\widehat{\beta}\bigr\Vert_{C^{-1}}^{2}\,.
\]

\end_inset

It follows immediately from the mean of a 
\begin_inset Formula $\chi^{2}(n-m)$
\end_inset

 distribution, which is simply 
\begin_inset Formula $n-m$
\end_inset

, that this estimator is unbiased: 
\begin_inset Formula $\mathbb{E}[\widehat{\sigma}^{2}]=\sigma^{2}$
\end_inset

.
 We remark that many elementary texts on linear regression fail to explain
 intuitively why the 
\begin_inset Quotes eld
\end_inset

sum of squares
\begin_inset Quotes erd
\end_inset

 needs to be divided by 
\begin_inset Formula $n-m$
\end_inset

.
 In our geometric approach, the subtraction of 
\begin_inset Formula $m$
\end_inset

 clearly comes from the 
\begin_inset Formula $m$
\end_inset

 
\begin_inset Quotes eld
\end_inset

degrees of freedom
\begin_inset Quotes erd
\end_inset

 (number of dimensions of randomness) being moved from the span 
\begin_inset Formula $M^{\perp}$
\end_inset

 of the residuals, into the span 
\begin_inset Formula $M$
\end_inset

 of the parameter estimators.
\end_layout

\begin_layout Standard
We will end this section by noting that the mean and variance of 
\begin_inset Formula $\widehat{\sigma}^{2}$
\end_inset

 could be computed from the covariance 
\begin_inset Formula $C$
\end_inset

 directly, without assuming 
\begin_inset Formula $\varepsilon$
\end_inset

 has a normal distribution.
 Also the estimators 
\begin_inset Formula $K\widehat{\beta}$
\end_inset

 and 
\begin_inset Formula $\widehat{\sigma}^{2}$
\end_inset

 remain uncorrelated if 
\begin_inset Formula $\varepsilon$
\end_inset

 is not normal.
 So the linear regression model could still be applied (in a more limited
 way) to observations with non-normal errors.
\end_layout

\begin_layout Section*
Scrap
\end_layout

\begin_layout Standard
The same arguments show that 
\begin_inset Formula $\bigl\Vert K\widehat{\beta}\bigr\Vert_{C^{-1}}^{2}$
\end_inset

 has a 
\begin_inset Formula $\chi^{2}(m)$
\end_inset

 distribution.
\end_layout

\begin_layout Standard
Most applications of the linear regression model in practice include, as
 part of the parameters, an 
\begin_inset Quotes eld
\end_inset

additive constant
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mu=\beta_{1}$
\end_inset

 such that 
\begin_inset Formula $\mathbb{E\langle}Y,e_{k}\rangle=\mu$
\end_inset

 for the standard basis 
\begin_inset Formula $e_{1},\dotsc,e_{n}$
\end_inset

.
 If 
\begin_inset Formula $Y$
\end_inset

 are the response/output variables in an experiment of 
\begin_inset Formula $n$
\end_inset

 trials assuming errors are homogeneous (so 
\begin_inset Formula $C$
\end_inset

 is the identity operator), then 
\begin_inset Formula $\mu$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

unconditional mean
\begin_inset Quotes erd
\end_inset

, while the other components of 
\begin_inset Formula $\beta$
\end_inset

 specify the sensitivity of the output to the inputs which are specified/designe
d as part of 
\begin_inset Formula $K$
\end_inset

.
 (The matrix of 
\begin_inset Formula $K$
\end_inset

 is often called the 
\begin_inset Quotes eld
\end_inset

design matrix
\begin_inset Quotes erd
\end_inset

.) 
\end_layout

\begin_layout Standard
This unconditional mean 
\begin_inset Formula $\mu$
\end_inset

 can be estimated by the sample mean, naturally:
\begin_inset Formula 
\[
\overline{Y}=\frac{1}{n}\sum_{k=1}^{n}Y_{k}\,.
\]

\end_inset

In the abstract formulation, to make 
\begin_inset Formula $\mathbb{E}\langle Y,e_{k}\rangle=\mathbb{E}\widehat{Y}$
\end_inset

 for all 
\begin_inset Formula $k$
\end_inset

, we want the vector 
\begin_inset Formula $(1,\dotsc,1)=\sum_{k=1}^{n}e_{k}$
\end_inset

 to be in 
\begin_inset Formula $M$
\end_inset

.
 It is specified as 
\begin_inset Formula $Ke_{1}$
\end_inset

.
 We make the orthogonalized version of this vector be the first basis vector
 of 
\begin_inset Formula $M$
\end_inset

:
\begin_inset Formula 
\[
v_{1}=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}e_{k}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X$
\end_inset

 is not assumed normal, we get a weaker result that is still useful to know.
 Like in the linear regression model, our previous calculations with the
 projections 
\begin_inset Formula $P_{M}$
\end_inset

 and 
\begin_inset Formula $P_{M^{\perp}}$
\end_inset

 remain valid without normality.
 Without loss of generality
\begin_inset Foot
status open

\begin_layout Plain Layout
This assumption gets rid of the additive constants involving 
\begin_inset Formula $\mathbb{E}X$
\end_inset

 and 
\begin_inset Formula $\mathbb{E}Y$
\end_inset

 in the predictors 
\begin_inset Formula $H$
\end_inset

 and 
\begin_inset Formula $H_{0}$
\end_inset

.
 The constants can always be added back, since doing so does not affect
 the following calculation of the variance of the prediction error.
\end_layout

\end_inset

, assume 
\begin_inset Formula $\mathbb{E}X=0$
\end_inset

.
 Let 
\begin_inset Formula $B_{0}=A^{\logof}(AA^{\logof})^{-1}$
\end_inset

 be the formula from eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:projection-adjoint"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for 
\begin_inset Quotes eld
\end_inset

predicting
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $x$
\end_inset

 from the input condition 
\begin_inset Formula $y=Ax$
\end_inset

.
 This predictor is linear even though, in general, 
\begin_inset Formula $\mathbb{E}[X\mid Y]$
\end_inset

 need not be linear in 
\begin_inset Formula $Y$
\end_inset

.
 But we can demonstrate, among all 
\emph on
linear
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
For any random variable 
\begin_inset Formula $X$
\end_inset

 in 
\begin_inset Formula $L^{2}$
\end_inset

 (i.e.
 with finite variance), 
\begin_inset Formula $\mathbb{E}[X\mid Y$
\end_inset

] has the much stronger property that it minimizes the variance of prediction
 error across 
\emph on
all
\emph default
 
\begin_inset Formula $L^{2}$
\end_inset

 functions 
\begin_inset Formula $g(Y)$
\end_inset

 of 
\begin_inset Formula $Y$
\end_inset

.
 That immediately follows from the well-known identity: 
\begin_inset Formula $\Var Z=\Var\bigl(\mathbb{E}[Z\mid Y]\bigr)+\mathbb{E}[\Var(Z\mid Y)\bigr]$
\end_inset

, substituting 
\begin_inset Formula $Z=X-g(Y)$
\end_inset

.
 Actually, 
\begin_inset Formula $\mathbb{E}[X\mid Y$
\end_inset

] can be realized as a certain orthogonal projection of 
\begin_inset Formula $Z$
\end_inset

, in an infinite-dimensional Hilbert space.
\end_layout

\end_inset

 predictors 
\begin_inset Formula $B\colon W\to V$
\end_inset

, the predictor 
\begin_inset Formula $B_{0}$
\end_inset

 is the best at minimizing the variance of the error in prediction, in every
 direction 
\begin_inset Formula $v\in V$
\end_inset

.
 
\end_layout

\begin_layout Standard
Intuitively our proposition should be true because 
\begin_inset Formula $x=B_{0}y$
\end_inset

 is the unique solution to the equation 
\begin_inset Formula $Ax=y$
\end_inset

 that minimizes 
\begin_inset Formula $\lVert x\rVert_{C^{-1}}^{2}$
\end_inset

.
 More formally, we may compute:
\begin_inset Formula 
\begin{align*}
\Var\bigl\langle v,X-BY\bigr\rangle=\Var\bigl\langle v,(I-BA)X\bigr\rangle & =\Var\bigl\langle(I-BA)^{*}v,X\bigr\rangle\\
 & =\bigl\Vert(I-BA)^{*}v\bigr\Vert_{C}^{2}\\
 & =\bigl\Vert C^{-1}(I-BA)^{\logof}Cv\bigr\Vert_{C}^{2}\\
 & =\bigl\Vert(I-BA)^{\logof}u\bigr\Vert{}_{C^{-1}}^{2}\,,\quad u=Cv\,,\\
 & =\bigl\Vert P_{M}(I-BA)^{\logof}u\bigr\Vert{}_{C^{-1}}^{2}+\bigl\Vert P_{M^{\perp}}(I-BA)^{\logof}u\bigr\Vert{}_{C^{-1}}^{2}\,.
\end{align*}

\end_inset

On the third line, and below, we are using the same notation of 
\begin_inset Formula $\cdot^{\logof}$
\end_inset

 for the adjoint with respect to the inner product 
\begin_inset Formula $\langle,\rangle_{C^{-1}}.$
\end_inset


\end_layout

\begin_layout Standard
We can simplify the projections:
\begin_inset Formula 
\[
P_{M^{\perp}}(I-BA)^{\logof}=P_{M^{\perp}}(I-A^{\logof}B^{\logof})=P_{M^{\perp}}\,,\quad\text{since \ensuremath{P_{M^{\perp}}A^{\logof}=0\,,}}
\]

\end_inset

and
\begin_inset Formula 
\begin{align*}
P_{M}(I-BA)^{\logof}=P_{M}(I-A^{\logof}B^{\logof})=P_{M}-A^{\logof}B^{\logof} & =P_{M}-(BA)^{\logof}\,,\quad\text{since \ensuremath{P_{M}A^{\logof}=A^{\logof}\,,}}\\
 & =(P_{M}-BA)^{\logof}\,,\quad\text{since \ensuremath{P_{M}^{\logof}=P_{M}\,,}}\\
 & =\bigl[(B_{0}-B)A\bigr]^{\logof}\,,\quad\text{since \ensuremath{P_{M}=B_{0}A\,.}}
\end{align*}

\end_inset

The last mapping vanishes identically by setting 
\begin_inset Formula $B=B_{0}$
\end_inset

.
 Therefore,
\begin_inset Formula 
\[
\Var\bigl\langle v,X-BY\bigr\rangle=\bigl\Vert\bigl[(B_{0}-B)A\bigr]^{\logof}u\bigr\Vert{}_{C^{-1}}^{2}+\bigl\Vert P_{M^{\perp}}u\bigr\Vert{}_{C^{-1}}^{2}\geq\bigl\Vert P_{M^{\perp}}Cv\bigr\Vert{}_{C^{-1}}^{2}\,,
\]

\end_inset

with the lower bound attained for all 
\begin_inset Formula $v\in V$
\end_inset

 when 
\begin_inset Formula $B=B_{0}$
\end_inset

, as claimed.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:conditional-expectation"

\end_inset

Application: expectation conditional on a linear transform
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X$
\end_inset

 be a 
\begin_inset Formula $V$
\end_inset

-valued Gaussian variable, and 
\begin_inset Formula $A\colon V\to W$
\end_inset

 be a surjective linear transformation to a (smaller) vector space 
\begin_inset Formula $W$
\end_inset

.
 We wish to compute the conditional expectation:
\begin_inset Formula 
\[
\mathbb{E}[X\mid Y]\,,\quad Y=AX\,.
\]

\end_inset


\end_layout

\begin_layout Standard
To tackle this problem, imagine we want to 
\begin_inset Quotes eld
\end_inset

solve
\begin_inset Quotes erd
\end_inset

 for 
\begin_inset Formula $x\in V$
\end_inset

 in the equation 
\begin_inset Formula $Ax=y$
\end_inset

, given 
\begin_inset Formula $y\in W$
\end_inset

.
 The infinitely many solutions can be characterized as belonging to one
 certain coset of the null space 
\begin_inset Formula $A^{-1}\{0\}$
\end_inset

.
\end_layout

\begin_layout Standard
As in the derivation of the linear regression model, we want to split 
\begin_inset Formula $X$
\end_inset

 into two components, one component being essentially the same as 
\begin_inset Formula $AX$
\end_inset

, and the other component orthogonal to the first.
 Then the expectation of the second component conditional on 
\begin_inset Formula $AX$
\end_inset

 should be equal to the unconditional expectation, since the two components
 are independent.
\end_layout

\begin_layout Standard
In light of these observations, and eq.
 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:adjoint-complement"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we consider the linear subspace 
\begin_inset Formula $M$
\end_inset

 to be the range of the adjoint of 
\begin_inset Formula $A$
\end_inset

, not of 
\begin_inset Formula $A$
\end_inset

 itself.
 Since 
\begin_inset Formula $X$
\end_inset

 may have a non-trivial covariance operator 
\begin_inset Formula $C$
\end_inset

, the adjoint of 
\begin_inset Formula $A$
\end_inset

 should be taken with respect to the 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

 inner product, assuming 
\begin_inset Formula $C$
\end_inset

 is non-singular.
 We will denote such an adjoint by 
\begin_inset Formula $A^{\logof}$
\end_inset

, to disambiguate it from the adjoint 
\begin_inset Formula $A^{*}$
\end_inset

 with respect to the standard inner product; we will be working with both
 in this section.
\end_layout

\begin_layout Standard
So 
\begin_inset Formula $V$
\end_inset

 is decomposed as the direct sum 
\begin_inset Formula 
\[
V=M\oplus M^{\perp}\,,\quad M=A^{\logof}V\,,\quad M^{\perp}=A^{-1}\{0\}\,,
\]

\end_inset

with the orthogonal complement taken with respect to 
\begin_inset Formula $\langle,\rangle_{C^{-1}}$
\end_inset

.
 Then by the result of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:independence-orthogonal-3"
plural "false"
caps "false"
noprefix "false"

\end_inset

, our Gaussian random variable is also decomposed:
\begin_inset Formula 
\[
X=P_{M}X+P_{M^{\perp}}X=P_{M}X+(I-P_{M})X
\]

\end_inset

into independent 
\begin_inset Formula $V$
\end_inset

-valued random variables, generated through the orthogonal projections 
\begin_inset Formula $P_{M}$
\end_inset

 and 
\begin_inset Formula $P_{M^{\perp}}$
\end_inset

, to 
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $M^{\perp}$
\end_inset

 respectively.
\end_layout

\begin_layout Standard
Now consider any 
\begin_inset Formula $x\in V$
\end_inset

.
 It has the representation:
\begin_inset Formula 
\[
x=P_{M}(x)+P_{M^{\perp}}(x)=A^{\logof}w+z\,,\quad w\in W\,,\quad z\in V\text{ such that }Az=0\,.
\]

\end_inset

We multiply this equation by 
\begin_inset Formula $A$
\end_inset

 on the left to relate it to (a given value of) 
\begin_inset Formula $Ax=y$
\end_inset

:
\begin_inset Formula 
\[
Ax=AA^{\logof}w\,.
\]

\end_inset

The operator 
\begin_inset Formula $AA^{\logof}\colon W\to W$
\end_inset

 is invertible: for 
\begin_inset Formula $A^{\logof}W$
\end_inset

 is orthogonal to 
\begin_inset Formula $A^{-1}\{0\}$
\end_inset

, so for any 
\begin_inset Formula $w\in W$
\end_inset

, the vector 
\begin_inset Formula $v=A^{\logof}w\in A^{\logof}W$
\end_inset

 either has 
\begin_inset Formula $Av\neq0$
\end_inset

 or 
\begin_inset Formula $v=0$
\end_inset

.
 The latter case occurs only if 
\begin_inset Formula $w=0$
\end_inset

 because 
\begin_inset Formula $A^{\logof}$
\end_inset

 is injective, which in turn follows from applying eq.
 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:adjoint-complement"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with 
\begin_inset Formula $T=A^{\logof}$
\end_inset

, together with our assumption that 
\begin_inset Formula $A$
\end_inset

 is surjective.
\end_layout

\begin_layout Standard
Hence, we may indeed solve:
\begin_inset Formula 
\begin{align}
w & =(AA^{\logof})^{-1}Ax\,.\nonumber \\
P_{M}(x) & =A^{\logof}w=A^{\logof}(AA^{\logof})^{-1}Ax.\label{eq:projection-adjoint}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Substituting 
\begin_inset Formula $x=X$
\end_inset

, we obtain:
\begin_inset Formula 
\[
P_{M}X=A^{\logof}(AA^{\logof})^{-1}AX=A^{\logof}(AA^{\logof})^{-1}Y\,.
\]

\end_inset

This equation shows 
\begin_inset Formula $P_{M}X$
\end_inset

 is a function of 
\begin_inset Formula $Y$
\end_inset

.
 Multiplying the same equation by 
\begin_inset Formula $A$
\end_inset

 on the left, we have 
\begin_inset Formula $AP_{M}X=Y$
\end_inset

, thereby showing, in the other direction, that 
\begin_inset Formula $Y$
\end_inset

 is a function of 
\begin_inset Formula $P_{M}X$
\end_inset

.
 So conditioning an expectation on 
\begin_inset Formula $Y$
\end_inset

 is the same
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $P_{M}X$
\end_inset

 generate the same 
\begin_inset Formula $\sigma$
\end_inset

-algebra in the Kolmogorov definition of conditional probability.
\end_layout

\end_inset

 as conditioning on 
\begin_inset Formula $P_{M}X$
\end_inset

.
 We can thus calculate:
\begin_inset Formula 
\begin{align*}
\mathbb{E}[X\mid Y]=\mathbb{E}\bigl[X\mid P_{M}X\bigr] & =\mathbb{E}\bigl[P_{M}X+P_{M^{\perp}}X\mid P_{M}X\bigr]\\
 & =P_{M}X+\mathbb{E}\bigl[P_{M^{\perp}}X\bigr]\\
 & =P_{M}X+P_{M^{\perp}}\bigl(\mathbb{E}X\bigr)\\
 & =P_{M}X+\bigl(I-P_{M}\bigr)\bigl(\mathbb{E}X\bigr)\\
 & =\mathbb{E}X+P_{M}\bigl(X-\mathbb{E}X\bigr)\\
 & =\mathbb{E}X+A^{\logof}(AA^{\logof})^{-1}(Y-\mathbb{E}Y)\,.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In practical computations, we can use the 
\begin_inset Quotes eld
\end_inset

change of variables
\begin_inset Quotes erd
\end_inset

 formula, eq.
 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:adjoint-change"
plural "false"
caps "false"
noprefix "false"

\end_inset

, to express 
\begin_inset Formula $A^{\logof}$
\end_inset

 in terms of 
\begin_inset Formula $A^{*}$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

.
 It reads
\begin_inset Foot
status open

\begin_layout Plain Layout
In applying eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:adjoint-change"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for 
\begin_inset Formula $T=A$
\end_inset

 here, 
\begin_inset Formula $C$
\end_inset

 must swap places with 
\begin_inset Formula $C^{-1}$
\end_inset

 there, because 
\begin_inset Formula $T^{\logof}$
\end_inset

 there denotes the adjoint taken with 
\begin_inset Formula $C$
\end_inset

, while the adjoint is taken with 
\begin_inset Formula $C^{-1}$
\end_inset

 here.
\end_layout

\end_inset

 : 
\begin_inset Formula $A^{\logof}=CA^{*}C^{-1}$
\end_inset

.
 So:
\begin_inset Formula 
\begin{align*}
\mathbb{E}[X\mid Y] & =\mathbb{E}X+CA^{*}C^{-1}(ACA^{*}C^{-1})^{-1}(Y-\mathbb{E}Y)\\
 & =\mathbb{E}X+CA^{*}(ACA^{*})^{-1}(Y-\mathbb{E}Y)\,,\quad Y=AX\,.
\end{align*}

\end_inset

This formula is easier to remember if we observe that 
\begin_inset Formula $ACA^{*}$
\end_inset

 is the covariance operator of 
\begin_inset Formula $Y$
\end_inset

, while 
\begin_inset Formula $CA^{*}$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset


\emph on
cross-covariance
\emph default

\begin_inset Quotes erd
\end_inset

 map of 
\begin_inset Formula $Y$
\end_inset

 versus 
\begin_inset Formula $X$
\end_inset

.
 In an obvious notation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
\mathbb{E}[X\mid Y] & =\mathbb{E}X+C_{X,Y}\,C_{Y}^{-1}(Y-\mathbb{E}Y)\,.\label{eq:predictor-2}\\
C_{Y}\colon W\to W & \quad\text{defined by}\:\langle w,C_{Y}w\rangle=\Var\langle w,Y\rangle\,.\nonumber \\
C_{X,Y}\colon W\to V & \quad\text{defined by}\:\text{\ensuremath{\langle v,C_{X,Y}w\rangle=\Cov\bigl[\langle v,X\rangle,\langle w,Y\rangle\bigr]}\,.}\nonumber 
\end{alignat}

\end_inset

These formulas may be familiar when 
\begin_inset Formula $Y$
\end_inset

 is scalar-valued, meaning 
\begin_inset Formula $C_{Y}^{-1}$
\end_inset

 is just scalar division by 
\begin_inset Formula $\Var Y$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X$
\end_inset

 is not assumed normal, we get a weaker result that is still useful to know.
 Without loss of generality
\begin_inset Foot
status open

\begin_layout Plain Layout
This assumption gets rid of the additive constants involving 
\begin_inset Formula $\mathbb{E}X$
\end_inset

 and 
\begin_inset Formula $\mathbb{E}Y$
\end_inset

 in the predictors 
\begin_inset Formula $B$
\end_inset

 and 
\begin_inset Formula $B_{0}$
\end_inset

.
 The constants can always be added back, since doing so does not affect
 
\begin_inset Formula $\Var\langle v,X-BY\rangle$
\end_inset

 below.
\end_layout

\end_inset

, assume 
\begin_inset Formula $\mathbb{E}X=0$
\end_inset

 and 
\begin_inset Formula $\mathbb{E}Y=0$
\end_inset

.
 Let 
\begin_inset Formula $B_{0}=C_{X,Y}\,C_{Y}^{-1}$
\end_inset

 be the linear transformation for 
\begin_inset Quotes eld
\end_inset

predicting
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $X$
\end_inset

 from the input condition 
\begin_inset Formula $Y$
\end_inset

.
 This predictor is linear even though, in general, 
\begin_inset Formula $\mathbb{E}[X\mid Y]$
\end_inset

 need not be linear in 
\begin_inset Formula $Y$
\end_inset

.
 But we can demonstrate, among all 
\emph on
linear
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
For any random variable 
\begin_inset Formula $X$
\end_inset

 in 
\begin_inset Formula $L^{2}$
\end_inset

 (i.e.
 with finite variance), 
\begin_inset Formula $\mathbb{E}[X\mid Y$
\end_inset

] has the much stronger property that it minimizes the variance of prediction
 error across 
\emph on
all
\emph default
 
\begin_inset Formula $L^{2}$
\end_inset

 functions 
\begin_inset Formula $g(Y)$
\end_inset

 of 
\begin_inset Formula $Y$
\end_inset

.
 That immediately follows from the well-known identity: 
\begin_inset Formula $\Var Z=\Var\bigl(\mathbb{E}[Z\mid Y]\bigr)+\mathbb{E}[\Var(Z\mid Y)\bigr]$
\end_inset

, substituting 
\begin_inset Formula $Z=X-g(Y)$
\end_inset

.
 Actually, 
\begin_inset Formula $\mathbb{E}[X\mid Y$
\end_inset

] can be realized as a certain orthogonal projection of 
\begin_inset Formula $Z$
\end_inset

, in an infinite-dimensional Hilbert space.
\end_layout

\end_inset

 predictors 
\begin_inset Formula $B\colon W\to V$
\end_inset

, the predictor 
\begin_inset Formula $B_{0}$
\end_inset

 is the best at minimizing the variance of the error in prediction, in every
 direction 
\begin_inset Formula $v\in V$
\end_inset

.
 Stated in this abstract form, the random variable 
\begin_inset Formula $Y$
\end_inset

 can be arbitrary (as long as it has finite variance) and does not have
 to be a linear transformation of 
\begin_inset Formula $X$
\end_inset

!
\end_layout

\begin_layout Standard
Fix 
\begin_inset Formula $v\in V$
\end_inset

 and let 
\begin_inset Formula $B\colon W\to V$
\end_inset

 vary.
 The variance of error in direction 
\begin_inset Formula $v$
\end_inset

 can be expanded like so:
\begin_inset Formula 
\begin{align*}
\Var\langle v,X-BY\rangle & =\Cov\bigl[\langle v,X-BY\rangle,\langle v,X-BY\rangle\bigr]\\
 & =\Var\langle v,X\rangle-2\Cov\bigl[\langle v,X\rangle,\langle v,BY\rangle\bigr]+\Var\langle v,BY\rangle\\
 & =\Var\langle v,X\rangle-2\Cov\bigl[\langle v,X\rangle,\langle B^{*}v,Y\rangle\bigr]+\Var\langle B^{*}v,Y\rangle\\
 & =\langle v,Cv\rangle-2\langle v,C_{X,Y}B^{*}v\rangle+\langle B^{*}v,C_{Y}B^{*}v\rangle\\
 & =\langle v,v\rangle_{C}-2\langle C_{Y}^{-1}C_{X,Y}^{*}v,B^{*}v\rangle_{C_{Y}}+\langle B^{*}v,B^{*}v\rangle_{C_{Y}}\\
 & =\bigl\Vert v\bigr\Vert_{C}^{2}-2\langle B_{0}^{*}v,B^{*}v\rangle_{C_{Y}}+\bigl\Vert B^{*}v\bigr\Vert_{C_{Y}}^{2}\,.
\end{align*}

\end_inset

The last expression is a quadratic form in 
\begin_inset Formula $B^{*}v$
\end_inset

.
 We can 
\begin_inset Quotes eld
\end_inset

complete the square
\begin_inset Quotes erd
\end_inset

 on it in analogy to scalar quadratic polynomials:
\begin_inset Formula 
\begin{align*}
\Var\langle v,X-BY\rangle & =\bigl\Vert v\bigr\Vert_{C}^{2}+\bigl\Vert B^{*}v-B_{0}^{*}v\bigr\Vert_{C_{Y}}^{2}-\bigl\Vert B_{0}^{*}v\bigr\Vert_{C_{Y}}^{2}\geq\bigl\Vert v\bigr\Vert_{C}^{2}-\bigl\Vert B_{0}^{*}v\bigr\Vert_{C_{Y}}^{2}\,.
\end{align*}

\end_inset

The lower bound is attained for all 
\begin_inset Formula $v\in V$
\end_inset

 when 
\begin_inset Formula $B=B_{0}$
\end_inset

, as claimed.
\end_layout

\begin_layout Section
Application: Kalman filtering
\end_layout

\begin_layout Standard
The 
\emph on
Kalman filter
\emph default
 iteratively estimates a sequence of random variables 
\begin_inset Formula $X_{1}$
\end_inset

,
\begin_inset Formula $X_{2},\dotsc,$
\end_inset

 taking values in a real vector space, given a known affine recursive relation
 between 
\begin_inset Formula $X_{j}$
\end_inset

 and 
\begin_inset Formula $X_{j+1}$
\end_inset

, along with a separate sequence of observations 
\begin_inset Formula $Y_{1},Y_{2},\dotsc$
\end_inset

 whose individual elements 
\begin_inset Formula $X_{j}$
\end_inset

 are linearly derived from the corresponding 
\begin_inset Formula $X_{j}$
\end_inset

.
 The random variables 
\begin_inset Formula $X_{j}$
\end_inset

 are interpreted as representing the hidden state of some system at times
 
\begin_inset Formula $j=1,2,\dotsc$
\end_inset

.
 We can only observe some transformation or projection 
\begin_inset Formula $Y_{j}$
\end_inset

 and must try to find out what the realizations 
\begin_inset Formula $X_{j}$
\end_inset

 are.
 At the same time, the evolution of both 
\begin_inset Formula $X_{j}$
\end_inset

 and 
\begin_inset Formula $Y_{j}$
\end_inset

 are subject to random 
\begin_inset Quotes eld
\end_inset

noise
\begin_inset Quotes erd
\end_inset

 which may be modelled as Gaussian
\begin_inset Foot
status open

\begin_layout Plain Layout
Actually, normality need not be assumed.
 The results we use from §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conditional-expectation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 have been shown, towards the end of that section, to apply to non-Gaussian
 probabilities as well.
 Without assuming a Gaussian law of errors, the Kalman filter computes a
 best least-squares estimator.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Let us begin by establishing the mathematical notation more precisely.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X_{1},X_{2},\dotsc$
\end_inset

 be random variables taking values in a finite-dimensional inner product
 space 
\begin_inset Formula $V$
\end_inset

.
 They represent the 
\emph on
hidden state
\emph default
 of the system at increasing time points.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X_{0}=x_{0}\in V$
\end_inset

 be some known or assumed initial state of the system.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X_{j}$
\end_inset

 evolve from 
\begin_inset Formula $X_{j-1}$
\end_inset

 according to:
\begin_inset Formula 
\[
X_{j}=\Psi_{j}X_{j-1}+\mu_{j}+\delta_{j}\,,\quad j=1,2,\dotsc,
\]

\end_inset

where 
\begin_inset Formula $\Psi_{j}\colon V\to V$
\end_inset

 is a known linear operator, 
\begin_inset Formula $\mu_{j}\in V$
\end_inset

 is a known perturbation or introduced force on the system, and 
\begin_inset Formula $\delta_{j}$
\end_inset

 is 
\begin_inset Formula $V$
\end_inset

-valued Gaussian noise with zero mean and known variance operator 
\begin_inset Formula $Q_{j}\colon V\to V$
\end_inset

.
 The random variables 
\begin_inset Formula $\delta_{j}$
\end_inset

 model the 
\emph on
deviation or noise in the underlying processes
\emph default
 driving 
\begin_inset Formula $X_{j}$
\end_inset

.
 The transformation 
\begin_inset Formula $\Psi_{j}$
\end_inset

 models how the hidden state evolves as time goes by.
 
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $Y_{j}$
\end_inset

 be random variables taking values in a finite-dimensional inner product
 space 
\begin_inset Formula $W$
\end_inset

.
 They represent the 
\emph on
observed state
\emph default
 of the system.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y_{j}$
\end_inset

 are defined by:
\begin_inset Formula 
\[
Y_{j}=A_{j}X_{j}+\varepsilon_{j}\,,
\]

\end_inset

for known linear transformations 
\begin_inset Formula $A_{j}\colon V\to W$
\end_inset

, and 
\begin_inset Formula $W$
\end_inset

-valued Gaussian noise 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 with zero mean and known variance operator 
\begin_inset Formula $R_{j}\colon W\to W$
\end_inset

.
 The random variables 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 model the 
\emph on
error or noise in observing or measuring
\emph default
 the state of the system.
\end_layout

\begin_layout Itemize
To concisely refer to what is known about the system at time 
\begin_inset Formula $j$
\end_inset

, we define 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

 to be the 
\begin_inset Formula $\sigma$
\end_inset

-algebra generated by 
\begin_inset Formula $Y_{1},\dotsc,Y_{j}$
\end_inset

 .
 For 
\begin_inset Formula $\mathcal{F}_{0}$
\end_inset

, set it to the 
\begin_inset Formula $\sigma$
\end_inset

-algebra consisting only of 
\begin_inset Formula $\emptyset$
\end_inset

 and its complement, representing the trivial information known at the start.
 These 
\begin_inset Formula $\sigma$
\end_inset

-algebras are increasing with time: 
\begin_inset Formula $\mathcal{F}_{0}\subset\mathcal{F}_{1}\subset\mathcal{F}_{2}\subset\dotsb$
\end_inset

.
\end_layout

\begin_layout Itemize
The random noises 
\begin_inset Formula $\delta_{j}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 shall be mutually independent.
 Thus, in particular 
\begin_inset Formula $\delta_{j}$
\end_inset

 and 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 are independent of 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

.
\end_layout

\begin_layout Standard
To give a concrete example, Kalman filtering might be applied in 
\emph on
navigation by dead-reckoning
\emph default
.
 Imagine we have a vehicle we must navigate.
 We know our spatial position & velocity 
\begin_inset Formula $x_{0}$
\end_inset

 at the start, but must continually estimate our position & velocity 
\begin_inset Formula $X_{j}$
\end_inset

 as we make navigational inputs, with incremental effect 
\begin_inset Formula $\mu_{j}$
\end_inset

 on the vehicle, while we might only be able to observe our current velocity
 
\begin_inset Formula $Y_{j}$
\end_inset

 (so 
\begin_inset Formula $A_{j}$
\end_inset

 is a projection of 
\begin_inset Formula $X_{j}$
\end_inset

 that drops the component of absolute position).
 The random variables 
\begin_inset Formula $\delta_{j}$
\end_inset

 represent error or deviations in physically controlling the vehicle or
 other unaccounted forces (e.g.
 friction) acting on the vehicle, while 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 represent measurement error of the velocity of the vehicle.
\end_layout

\begin_layout Standard
We note that the Kalman filter can be generalized slightly to allow quantities
 like 
\begin_inset Formula $u_{j}$
\end_inset

 to be random, or to be dependent on preceding observations, and for 
\begin_inset Formula $Y_{j}$
\end_inset

 to incorporate random/affine perturbations as well.
 If their uncertainty can be fully resolved by 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

 (at time 
\begin_inset Formula $j-1$
\end_inset

), we can treat them as known without affecting anything else.
 For notational simplicity, we will not detail such generalization further
 in this section.
\end_layout

\begin_layout Standard
Under our more abstract presentation of the mathematics, we aim to explicitly
 display a formula for the conditional expectation
\begin_inset Formula 
\[
\widehat{X}_{j}=\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j}\bigr]\,,\quad j=1,2,\dotsc,
\]

\end_inset

which we take as the 
\emph on
(best) (linear) estimator
\emph default
 of 
\begin_inset Formula $X_{j}$
\end_inset

 given 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

.
 The derivation turns out to be a straightforward, if notationally-heavy,
 application of the results of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conditional-expectation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
To begin, we fix a value for 
\begin_inset Formula $j$
\end_inset

 and consider the random variables 
\begin_inset Formula $X_{1},X_{2},\dotsc,X_{j}$
\end_inset

 and 
\begin_inset Formula $Y_{1},\dotsc,Y_{j}$
\end_inset

 under the probability law
\begin_inset Foot
status open

\begin_layout Plain Layout
Since 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

 is generated by a finite set of random variables taking values in a finite-dime
nsional space, a conditional probability measure consistent with the Kolmogorov
 theory of probability can be readily constructed, without the complications
 that commonly ensue from filtrations that cannot be finitely generated.
 
\end_layout

\end_inset

 
\emph on
after conditioning under 
\emph default

\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

.
 Under such a probability law, conditioning any expectations, variances
 and covariances further on the random variable 
\begin_inset Formula $Y_{j}$
\end_inset

 will be equivalent to conditioning on 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

 originally.
 With this set-up, eq.
 
\begin_inset CommandInset ref
LatexCommand vref
reference "eq:predictor-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be applied to expand:
\begin_inset Formula 
\[
\widehat{X}_{j}=\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j}\bigr]=\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j-1}\bigr]+C_{XY,j}C_{Yj}^{-1}\,\bigl(Y_{j}-\mathbb{E}\bigl[Y_{j}\mid\mathcal{F}_{j-1}\bigr]\bigr)
\]

\end_inset


\end_layout

\begin_layout Standard
with:
\end_layout

\begin_layout Itemize
\begin_inset Formula $C_{Y,j}(\omega)\colon W\to W$
\end_inset

 being the variance operator
\begin_inset Foot
status open

\begin_layout Plain Layout
Conditional variances are 
\size normal
\emph on
random variables
\emph default
 (measurable with respect to 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

 or 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

 in our case) under the Kolmogorov theory of probability, so 
\begin_inset Formula $C_{Y,j}$
\end_inset

 is really a random variable whose realization 
\begin_inset Formula $C_{Y,j}(\omega)$
\end_inset

, for each sample point 
\begin_inset Formula $\omega$
\end_inset

 in the probability space, is a linear operator 
\begin_inset Formula $W\to W$
\end_inset

.
 Thus we would be formally wrong to write the intuitive notation 
\begin_inset Formula $C_{Y,j}\colon W\to W$
\end_inset

.
 However, as with most works dealing with probability theory, 
\begin_inset Formula $\omega$
\end_inset

 is mostly irrelevant and will be suppressed in our notation.
 These comments apply similarly to the objects 
\begin_inset Formula $C_{XY,j}$
\end_inset

, 
\begin_inset Formula $S_{j}$
\end_inset

 and 
\begin_inset Formula $\widehat{S}_{j}$
\end_inset

 introduced later in this section.
\end_layout

\end_inset

 of 
\begin_inset Formula $Y_{j}$
\end_inset

 conditional on 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

, defined on test vectors 
\begin_inset Formula $w\in W$
\end_inset

 by 
\begin_inset Formula 
\[
\langle w,C_{Y,j}w\rangle=\Var\bigl[\langle w,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,;\quad\text{and}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $C_{XY,j}(\omega)\colon W\to V$
\end_inset

 being the covariance homomorphism of 
\begin_inset Formula $X_{j}$
\end_inset

 versus 
\begin_inset Formula $Y_{j}$
\end_inset

 conditional on 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

, defined on test vectors 
\begin_inset Formula $v\in V$
\end_inset

, 
\begin_inset Formula $w\in W$
\end_inset

 by
\begin_inset Formula 
\[
\langle v,C_{XY,j}w\rangle=\Cov\bigl[\langle v,X_{j}\rangle,\langle w,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,.
\]

\end_inset


\end_layout

\begin_layout Standard
As in near the end of §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conditional-expectation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we further introduce the notation:
\begin_inset Formula 
\[
B_{j}=C_{XY,j}C_{Y,j}^{-1}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
Then, continuing to expand on 
\begin_inset Formula $\widehat{X}_{j}$
\end_inset

, we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\widehat{X}_{j} & =\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j-1}\bigr]+B_{j}\bigl(Y_{j}-\mathbb{E}\bigl[A_{j}X_{j}+\varepsilon_{j}\mid\mathcal{F}_{j-1}\bigr]\bigr)\nonumber \\
 & =\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j-1}\bigr]+B_{j}\bigl(Y_{j}-A_{j}\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j-1}\bigr]\bigr)\nonumber \\
 & =\bigl(I-B_{j}A_{j}\bigr)\mathbb{E}\bigl[X_{j}\mid\mathcal{F}_{j-1}\bigr]+B_{j}Y_{j}\nonumber \\
 & =\bigl(I-B_{j}A_{j}\bigr)\mathbb{E}\bigl[\Psi_{j}X_{j-1}+\mu_{j}+\delta_{j}\mid\mathcal{F}_{j-1}\bigr]+B_{j}Y_{j}\nonumber \\
 & =\bigl(I-B_{j}A_{j}\bigr)\bigl(\Psi_{j}\widehat{X}_{j-1}+\mu_{j}\bigr)+B_{j}Y_{j}\,.\label{eq:kalman-predictor}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
It remains to exhibit computable formulas for 
\begin_inset Formula $C_{Y,j}$
\end_inset

 and 
\begin_inset Formula $C_{XY,j}$
\end_inset

.
 
\end_layout

\begin_layout Standard
We tackle 
\begin_inset Formula $C_{Y,j}$
\end_inset

 first:
\begin_inset Formula 
\begin{align}
\langle w,C_{Y,j}w\rangle & =\Var\bigl[\langle w,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\nonumber \\
 & =\Var\bigl[\langle w,A_{j}X_{j}+\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\nonumber \\
 & =\Var\bigl[\langle w,A_{j}X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\bigl[\langle w,\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,.\label{eq:kalman-cond-variance-obs}
\end{align}

\end_inset

The last term on eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-cond-variance-obs"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is easy to recognize.
 
\begin_inset Formula $\Var\bigl[\langle w,\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]$
\end_inset

 is the same as 
\begin_inset Formula $\Var\langle w,\varepsilon_{j}\rangle$
\end_inset

 because 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 is completely independent of 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

.
 And 
\begin_inset Formula $\Var\langle w,\varepsilon_{j}\rangle$
\end_inset

, by definition, is the variance of 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 evaluated (as a quadratic form) at test vector 
\begin_inset Formula $w$
\end_inset

.
 Expressed in terms of the variance operator 
\begin_inset Formula $R_{j}$
\end_inset

 we have:
\begin_inset Formula 
\[
\Var\bigl[\langle w,\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]=\Var\langle w,\varepsilon_{j}\rangle=\langle w,R_{j}w\rangle\,.
\]

\end_inset


\end_layout

\begin_layout Standard
On the other hand, the first term on eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-cond-variance-obs"
plural "false"
caps "false"
noprefix "false"

\end_inset

 requires expanding 
\begin_inset Formula $X_{j}$
\end_inset

 by its recurrence relation with 
\begin_inset Formula $X_{j-1}$
\end_inset

, like inside 
\begin_inset Formula $\widehat{X}_{j}$
\end_inset

 earlier:
\begin_inset Formula 
\begin{align*}
\Var\bigl[\langle w,A_{j}X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr] & =\Var\bigl[\langle w,A_{j}(\Psi_{j}X_{j-1}+\mu_{j}+\delta_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle w,A_{j}\Psi_{j}X_{j-1}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\bigl[\langle w,A_{j}\delta_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,.
\end{align*}

\end_inset

The last term above may be recognized as analogous to 
\begin_inset Formula $\Var\bigl[\langle w,\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]$
\end_inset

, but with 
\begin_inset Formula $A_{j}\delta_{j}$
\end_inset

 replacing 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

; it is obviously related to the variance operator 
\begin_inset Formula $Q_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
Unfortunately, our insistence in this document on only passing scalar arguments
 to 
\begin_inset Formula $\Var[\cdot]$
\end_inset

 obscures the simple concept behind the last equation: what happens to the
 variance (operator) of a vector-valued random variable after applying linear
 transformations.
 We already know the answer from §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:linear-transformations"
plural "false"
caps "false"
noprefix "false"

\end_inset

 — compose the original variance operator on the left by the transformation,
 and on the right by its adjoint — and do not need to repeat the derivations
 in detail.
 
\end_layout

\begin_layout Standard
We pause our analysis on eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-cond-variance-obs"
plural "false"
caps "false"
noprefix "false"

\end_inset

 to introduce the following simplifying notation for the conditional variances
 of the hidden state — which already have been seen lurking so far.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1,2,\dotsc,$
\end_inset

 let the (conditional variance) operator 
\begin_inset Formula $S_{j}(\omega)\colon V\to V$
\end_inset

 be defined by: 
\begin_inset Formula 
\[
\langle v,S_{j}v\rangle=\Var\bigl[\langle v,X_{j}\rangle\mid\mathcal{F}_{j}\bigr]\,,\quad v\in V\,.
\]

\end_inset

The special case 
\begin_inset Formula $S_{0}$
\end_inset

 is defined as the identically zero transformation, which is consistent
 with the above formula with 
\begin_inset Formula $j$
\end_inset

 set to 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $j=1,2,\dotsc,$
\end_inset

 let the (conditional variance) operator 
\begin_inset Formula $\widehat{S}_{j}(\omega)\colon V\to V$
\end_inset

 be defined by:
\begin_inset Formula 
\[
\langle v,\widehat{S}_{j}v\rangle=\Var\bigl[\langle v,X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,,\quad v\in V\,.
\]

\end_inset


\end_layout

\begin_layout Standard
The latter, 
\begin_inset Formula $\widehat{S}_{j}$
\end_inset

, exactly represents the variance of 
\begin_inset Formula $X_{j}$
\end_inset

 conditional on 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

, i.e.
 letting 
\begin_inset Formula $X_{j}$
\end_inset

 evolve from 
\begin_inset Formula $X_{j-1}$
\end_inset

 according to the established law but without having observed 
\begin_inset Formula $Y_{j}$
\end_inset

 yet.
 We may find a simple formula for it, proceeding as follows:
\begin_inset Formula 
\begin{align*}
\langle v,\widehat{S}_{j}\rangle & =\Var\bigl[\langle v,\Psi_{j}X_{j-1}+\mu_{j}+\delta_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle v,\Psi_{j}X_{j-1}+\mu_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\bigl[\langle v,\delta_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle v,\Psi_{j}X_{j-1}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\langle v,\delta_{j}\rangle
\end{align*}

\end_inset

The first line involves the recurrence relation of 
\begin_inset Formula $X_{j}.$
\end_inset

 The second line follows because 
\begin_inset Formula $\delta_{j}$
\end_inset

 is independent of 
\begin_inset Formula $\Psi_{j}X_{j-1}+\mu_{j}$
\end_inset

 — even unconditionally, but all the more true under 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

.
 Then the third line follows because 
\begin_inset Formula $\mu_{j}$
\end_inset

 can be treated as a constant under 
\begin_inset Formula $\Var[\cdot\mid\mathcal{F}_{j-1}\bigr]$
\end_inset

, while 
\begin_inset Formula $\Var[\langle v,\delta_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]$
\end_inset

 simply equals 
\begin_inset Formula $\Var\langle v,\delta_{j}\rangle$
\end_inset

.
 
\end_layout

\begin_layout Standard
Observe the left term on the third line represents the conditional variance
 of 
\begin_inset Formula $X_{j-1}$
\end_inset

 after transformation by 
\begin_inset Formula $\Psi_{j}$
\end_inset

.
 Dropping the test vectors 
\begin_inset Formula $v\in V$
\end_inset

, we thus recognize this equality of linear operators:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widehat{S}_{j}=\Psi_{j}S_{j-1}\Psi_{j}^{*}+Q_{j}\,.
\]

\end_inset


\end_layout

\begin_layout Standard
Returning to eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-cond-variance-obs"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we may argue with the same reasoning to arrive at:
\begin_inset Formula 
\[
C_{Y,j}=A_{j}\widehat{S}_{j}A_{j}^{*}+R_{j}\,,
\]

\end_inset

after dropping out the test vectors 
\begin_inset Formula $w\in W$
\end_inset

.
\end_layout

\begin_layout Standard
Next we attack the covariance homomorphism 
\begin_inset Formula $C_{XY,j}(\omega)\colon W\to V$
\end_inset

, which turns out to be very easy:
\begin_inset Formula 
\begin{align*}
\langle v,C_{XY,j}w\rangle & =\Cov\bigl[\langle v,X_{j}\rangle,\langle w,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Cov\bigl[\langle v,X_{j}\rangle,\langle w,A_{j}X_{j}+\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Cov\bigl[\langle v,X_{j}\rangle,\langle w,A_{j}X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Cov\bigl[\langle v,X_{j}\rangle,\langle w,\varepsilon_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Cov\bigl[\langle v,X_{j}\rangle,\langle A_{j}^{*}w,X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+0\\
 & =\langle v,\widehat{S}_{j}A_{j}^{*}w\rangle\,.
\end{align*}

\end_inset

The fourth line follows from 
\begin_inset Formula $\varepsilon_{j}$
\end_inset

 being independent of 
\begin_inset Formula $X_{j}$
\end_inset

.
 Dropping the test vectors 
\begin_inset Formula $v\in V$
\end_inset

 and 
\begin_inset Formula $w\in W$
\end_inset

, we therefore see:
\begin_inset Formula 
\begin{equation}
C_{XY,j}=\widehat{S}_{j}A_{j}^{*}.\label{eq:kalman-covariance}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
To complete the specification of the Kalman filter as an algorithm, we must
 have a formula for the conditional variance 
\begin_inset Formula $S_{j}$
\end_inset

.
 As one might expect, it can be computed recursively.
 Actually, we have essentially computed it in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conditional-expectation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 already, under somewhat different notation.
 For clarity, we will repeat those arguments, adapted to the present situation.
 The main principle at work is the orthogonal decomposition of the (conditional)
 variance of the random variable 
\begin_inset Formula $X_{j}$
\end_inset

 as the sum of the variance of its least-squares predictor 
\begin_inset Formula $\widehat{X}_{j}$
\end_inset

 plus the variance of the residual 
\begin_inset Formula $X_{j}-\widehat{X}_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
Consider:
\begin_inset Formula 
\begin{align*}
\langle v,S_{j}v\rangle & =\Var\bigl[\langle v,X_{j}\rangle\mid\mathcal{F}_{j}\bigr]\\
 & =\Var\bigl[\langle v,(X_{j}-\widehat{X}_{j})+\widehat{X}_{j}\rangle\mid\mathcal{F}_{j}\bigr]\\
 & =\Var\bigl[\langle v,X_{j}-\widehat{X}_{j}\rangle\mid\mathcal{F}_{j}\bigr]\\
 & =\Var\bigl[\langle v,X_{j}-\widehat{X}_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle v,X_{j}-B_{j}Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\,.
\end{align*}

\end_inset

The third line follows because the predictor 
\begin_inset Formula $\widehat{X}_{j}$
\end_inset

 is a 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

-measurable function (i.e.
 a function of 
\begin_inset Formula $Y_{1},\dotsc,Y_{j})$
\end_inset

, so it can be treated like a constant when conditioning under 
\begin_inset Formula $\mathcal{F}_{j}$
\end_inset

; the variance of a constant is zero.
 The fourth line simply removes the variable 
\begin_inset Formula $Y_{j}$
\end_inset

 from the conditioning, which is allowed because the residual 
\begin_inset Formula $X_{j}-\widehat{X}_{j}$
\end_inset

 (under the conditional probability law of 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

) is orthogonal to all linear functions of 
\begin_inset Formula $Y_{j}$
\end_inset

, as proven in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conditional-expectation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Finally, the fifth line comes from a casual observation of eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-predictor"
plural "false"
caps "false"
noprefix "false"

\end_inset

: 
\begin_inset Formula $\widehat{X}_{j}$
\end_inset

 is the sum of a 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

-measurable function plus 
\begin_inset Formula $B_{j}Y_{j}$
\end_inset

; the former has zero variance when conditioning under 
\begin_inset Formula $\mathcal{F}_{j-1}$
\end_inset

.
\end_layout

\begin_layout Standard
Continuing from the last line, we have:
\begin_inset Formula 
\begin{align*}
\langle v,S_{j}v\rangle & =\Cov\bigl[\langle v,X_{j}-B_{j}Y_{j}\rangle,\langle v,X_{j}-B_{j}Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle v,X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]-2\Cov\bigl[\langle v,X_{j}\rangle,\langle v,B_{j}Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\bigl[\langle v,B_{j}Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\Var\bigl[\langle v,X_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]-2\Cov\bigl[\langle v,X_{j}\rangle,\langle B_{j}^{*}v,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]+\Var\bigl[\langle B_{j}^{*}v,Y_{j}\rangle\mid\mathcal{F}_{j-1}\bigr]\\
 & =\langle v,\widehat{S}_{j}v\rangle-2\langle v,C_{XY,j}B_{j}^{*}v\rangle+\langle v,B_{j}C_{Y,j}B_{j}^{*}v\rangle\\
 & =\langle v,\widehat{S}_{j}v\rangle-\langle v,B_{j}C_{XY,j}^{*}v\rangle\\
 & =\langle v,\widehat{S}_{j}v\rangle-\langle v,B_{j}(A_{j}\widehat{S}_{j})v\rangle\,.
\end{align*}

\end_inset

The fifth line follows from substituting the definition 
\begin_inset Formula $B_{j}=C_{XY,j}C_{Y,j}^{-1}$
\end_inset

 and simplifying.
 The last line follows from substituting in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-covariance"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Dropping off the test vectors 
\begin_inset Formula $v\in V$
\end_inset

 immediately shows:
\begin_inset Formula 
\[
S_{j}=(I-B_{j}A_{j})\widehat{S}_{j}\,.
\]

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Axler"
key "Axler"
literal "false"

\end_inset

Sheldon Axler.
 
\emph on
Linear Algebra Done Right
\emph default
.
 Fourth Edition.
 Springer, 2024.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Billingsley"
key "Billingsley"
literal "false"

\end_inset

Patrick Billingsley.
 
\emph on
Probability and Measure
\emph default
.
 
\emph on
Second Edition
\emph default
.
 John Wiley & Sons, 1986.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Feller"
key "Feller"
literal "false"

\end_inset

William Feller.
 
\emph on
An Introduction to Probability Theory and Its Applications
\emph default
, 
\emph on
Volume II
\emph default
.
 John Wiley & Sons, 1970.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Folland"
key "Folland"
literal "false"

\end_inset

Gerald B.
 Folland.
 
\emph on
Real Analysis: Modern Techniques and Their Applications
\emph default
.
 
\emph on
Second Edition
\emph default
.
 John Wiley & Sons, 1999.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Friedberg"
key "Friedberg"
literal "false"

\end_inset

Stephen H.
 Friedberg, Arnold J.
 Insel, Lawrence E.
 Spence.
 
\emph on
Linear Algebra.
 Third Edition
\emph default
.
 Prentice Hall, 1997.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Kolmogorov"
key "Kolmogorov"
literal "false"

\end_inset

A.
 N.
 Kolmogorov.
 Trans.
 Nathan Morrison.
 
\emph on
Foundations of the Theory of Probability
\emph default
.
 Chelsea, 1956.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Rohatgi"
key "Rohatgi"
literal "false"

\end_inset

Vijay K.
 Rohatgi, A.
 K.
 MD.
 Ehsanes Saleh.
 
\emph on
An Introduction to Probability and Statistics.
 Second Edition
\emph default
.
 Prentice Hall, 2001.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Rosenthal"
key "Rosenthal"
literal "false"

\end_inset

Jeffrey S.
 Rosenthal.
 
\emph on
A First Look at Rigorous Probability Theory
\emph default
.
 World Scientific, 2003.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Wichura"
key "Wichura"
literal "false"

\end_inset

Michael J.
 Wichura.
 
\emph on
The Coordinate-Free Approach to Linear Models
\emph default
.
 Cambridge University Press, 2006.
\end_layout

\end_body
\end_document
